{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"01a1fb5f969f467c8ae5902aa8e11444":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24fc85afed3f4193b3c1b6da8eeb7736":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"24fe006192ba47beb1dbaf67461088c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89d2ee9ec5504f268e52c1397c73c297","placeholder":"â€‹","style":"IPY_MODEL_8826a2d8efe9492ab14e638cbf2bd166","value":"Connecting..."}},"3a7ffdf2ae0e4e67b0507a01b2953358":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c041926646343828078f168e296591d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4343dab244f640cea51a50cd4aeb2514":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_6fb30196b17e4709b0703a3c3a0eb4f7","style":"IPY_MODEL_7a819236a7724432b2004c25ef0a2507","value":true}},"6fb30196b17e4709b0703a3c3a0eb4f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a819236a7724432b2004c25ef0a2507":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8826a2d8efe9492ab14e638cbf2bd166":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8841c3d00f2046b18c68da674c74378d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89d2ee9ec5504f268e52c1397c73c297":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0868a65a8c4b789471c9c9bd806c3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_f755c96ca1a44ce58190a40f03905651"}},"8c86b5a112984b4f8e985cd1dc9a75fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_3a7ffdf2ae0e4e67b0507a01b2953358","style":"IPY_MODEL_24fc85afed3f4193b3c1b6da8eeb7736","tooltip":""}},"a0a59bda3e3e41849550823d4c5910fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c041926646343828078f168e296591d","placeholder":"â€‹","style":"IPY_MODEL_8841c3d00f2046b18c68da674c74378d","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a45b67bba38d45ba953a68b19eb65929":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4d4492a72344b7988bcf4c1675d4ea9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e79c19404921474b8604edfa1a959029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eee18a20463341abbf5eee7b2177367c","placeholder":"â€‹","style":"IPY_MODEL_a45b67bba38d45ba953a68b19eb65929","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"eee18a20463341abbf5eee7b2177367c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f755c96ca1a44ce58190a40f03905651":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"fd0915605e5f4c5380e302afa5a87d88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_01a1fb5f969f467c8ae5902aa8e11444","placeholder":"â€‹","style":"IPY_MODEL_b4d4492a72344b7988bcf4c1675d4ea9","value":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UCCD3074: Deep Learning for Data Science Group Assignment","metadata":{"id":"30ee1199-6735-4ee3-9130-6907041ed3ce"}},{"cell_type":"markdown","source":"### Trimester June 2025","metadata":{"id":"48433aa8-5cd2-49c4-8e8d-69f36581d841"}},{"cell_type":"markdown","source":"### Title: Fine-Tuning a Transformer for Detecting AI-Generated Text (Application-Based)","metadata":{"id":"5995ded1-4bf9-47ee-9e3a-d2200d856a11"}},{"cell_type":"markdown","source":"### Group 6:\n1. Charmaine Hooi Wai Yee (2104533)\n2. Khow Kai Yong (2105725)\n3. Michelle Koh Mei Xian (2103784)\n4. Seow Yi Xuan (2105524)","metadata":{"id":"82128b9f-52fc-413e-9fa7-fb2738804e0b"}},{"cell_type":"markdown","source":"## **1.0: Import Libraries**","metadata":{"id":"c063d1af-3361-4214-9a51-00703ded4038"}},{"cell_type":"code","source":"#1\n!pip install datasets --upgrade\n#2\n!pip install transformers --upgrade\n!pip install scikit-learn --upgrade","metadata":{"id":"FpXfd1qaqKjC","outputId":"90114def-06e4-4786-e01b-6f87fe7ebf96","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face\n# It will prompt to paste access token\n# Paste Hugging Face access token: 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\nfrom huggingface_hub import login\nlogin()","metadata":{"id":"oOE87PQYttX4","outputId":"43329da7-5cb2-40d0-8556-79f9bc42dd29","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git config --global credential.helper store","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Michelle Koh Mei Xian 2103784\n# ########################################\nfrom datasets import load_dataset\nimport pandas as pd","metadata":{"id":"15812921-4268-49ac-b98a-f5ba69e67287","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **2.0: Dataset Preparation & Cleaning**","metadata":{"id":"b6584c8f-bc1f-47f6-ae88-48d8934a150a"}},{"cell_type":"markdown","source":"#### 2.1: Load and Inspect The Dataset","metadata":{"id":"72890683-e2e7-4067-afa3-0fa4c697167b"}},{"cell_type":"markdown","source":"First, we loaded the Hugging Face dataset `andythetechnerd03/AI-human-text`, which contains short texts classified as either human-written or AI-generated. The training split was converted into a Pandas DataFrame to make it easier to work with.\n\nWe then checked the number of samples and how many were AI vs human.  `'Generated'` is the original label column, with 0 indicating human and 1 means AI. For better readability, we added a new column `'label_text'` to show the labels as \"human\" or \"ai\".","metadata":{"id":"ow6_NX4tyX2H"}},{"cell_type":"code","source":"# ########################################\n# Coded By: Michelle Koh Mei Xian 2103784\n# ########################################\n\n# load the dataset from Hugging Face\ndataset = load_dataset(\"andythetechnerd03/AI-human-text\")\nprint(dataset)\n\n# Convert the 'train' split into a pandas DataFrame for easier processing and inspection\ndf = pd.DataFrame(dataset['train'])\nprint(\"Initial DataFrame shape:\", df.shape)\n\n# inspect the dataset\nprint(df.head())                      # Show the first few samples\nprint(df['generated'].value_counts()) # Count the number of AI-generated (1) vs human-written (0) texts\n\n# Map label integers to human-readable text\nlabel_map = {0: \"human\", 1: \"ai\"}\ndf['label_text'] = df['generated'].map(label_map)","metadata":{"id":"cb54e291-8507-47c9-ac30-2f3659aeda2f","outputId":"208750e3-e46f-4f1f-b234-463eaae6161d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2.2: Balance Human vs AI Samples","metadata":{"id":"THOHf7qd0Ogn"}},{"cell_type":"markdown","source":"The model's learning may be affected by the fact that the initial dataset contained more human-written texts than AI-generated ones.  We corrected this by taking an equal number of samples from each class, balancing the dataset.  Due to the decreased class size, we employed undersampling, in which we chose an equal number of samples at random from each class.  This makes it easier for the model to learn both kinds of text equally.","metadata":{"id":"WMU-mLbs1H5j"}},{"cell_type":"code","source":"# ########################################\n# Coded By: Michelle Koh Mei Xian 2103784\n# ########################################\nmin_class_count = df['generated'].value_counts().min()\n\ndf_balanced = (\n    df.groupby('generated', group_keys=False)\n    .apply(lambda x: x.sample(min_class_count, random_state=42))\n    .reset_index(drop=True)\n)\n\nprint(\"Balanced Data Shape:\", df_balanced.shape)\nprint(df_balanced['generated'].value_counts())\n","metadata":{"id":"ilkGrXgC0kpe","outputId":"4585372b-801c-4221-95be-8957b4fe4b63","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2.3: Clean Text, Remove Duplicates, Handle Encoding Issues","metadata":{"id":"Z-vUJ8Dx1xpL"}},{"cell_type":"markdown","source":"We cleaned the text data by removing duplicate entries, blank texts, and any encoding issues. This step ensures the dataset is clean and ready for training.","metadata":{"id":"GHjvhU0I3FXs"}},{"cell_type":"code","source":"# ########################################\n# Coded By: Michelle Koh Mei Xian 2103784\n# ########################################\n# Remove duplicate text entries\nprint(\"Duplicate entries:\", df_balanced.duplicated(subset='text').sum())\ndf_balanced = df_balanced.drop_duplicates(subset='text')\n\n# Handle missing or blank text\ndf_balanced = df_balanced.dropna(subset=['text'])  # Drop NaN values\ndf_balanced['text'] = df_balanced['text'].astype(str).str.strip()  # Remove leading/trailing spaces\ndf_balanced = df_balanced[df_balanced['text'] != \"\"]  # Remove empty strings\n\n# Handle encoding issues\ndf_balanced['text'] = df_balanced['text'].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n\n# Final shape\nprint(\"Cleaned Data Shape:\", df_balanced.shape)\n\n# Save cleaned data\ndf_balanced.to_csv(\"balanced_ai_human_text.csv\", index=False)\nprint(\"Cleaned dataset saved as CSV.\")\n","metadata":{"id":"f6h1lb8N1zDT","outputId":"a96dc487-6801-4843-b300-57177ca8f8c5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2.4: Create a Reduced Sample","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n# Take a MUCH smaller sample\nsample_size = 60000  # Only 60,000 samples instead of 300,000+\ndf_small = df.sample(n=sample_size, random_state=42, replace=False)\n\nprint(f\"Reduced dataset: {df_small.shape}\")\nprint(\"Class distribution in small sample:\")\nprint(df_small['generated'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3.0 Tokenization and Input Formatting**","metadata":{}},{"cell_type":"markdown","source":"This stage is to convert the raw text into a numerical format that a machine learning model especially a transformer **(distilroBerta)** can understand.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Import Necessary Libraries\n","metadata":{}},{"cell_type":"markdown","source":"* AutoTokenizer: It is used to automatically loads the correct tokenizer based on a transformer model name\n* Dataset: Hugging Face's fast and memory-efficient dataset format\n* train_test_split: Split the data into training and validation set\n* DataLoader: Prepares batches for training in PyTorch","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.2 Load the Cleaned CSV Dataset","metadata":{}},{"cell_type":"markdown","source":"The cleaned and balanced CSV dataset created in Section 2.0 is loaded.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\ndf = pd.read_csv(\"balanced_ai_human_text.csv\")\nsample_size = 60000  # match Notebook (1)\nif len(df) > sample_size:\n    df = df.sample(n=sample_size, random_state=42, replace=False)\nprint(f\"Using {len(df)} rows for faster training (downsampled if > {sample_size}).\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.3 Split into Training and Validation Sets","metadata":{}},{"cell_type":"markdown","source":"1. The dataset is split into 80% of training data and 20% of validation data.\n2. The use of \"stratify\" is to ensure the equal distribution of AI and human classes in both splits","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['generated'].tolist(),\n    test_size=0.2, random_state=42, stratify=df['generated']\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n# ########################################\n# Prevent Data Leakage\n# Reason: Original split may contain the same text in both train and val.\n# Fix: Remove duplicates WITHIN each split after the split is done.\n# ########################################\n\n# 1. Combine the split texts and labels back into DataFrames\ntrain_df = pd.DataFrame({'text': train_texts, 'generated': train_labels})\nval_df = pd.DataFrame({'text': val_texts, 'generated': val_labels})\n\nprint(f\"Before deduplication - Train: {train_df.shape}, Val: {val_df.shape}\")\n\n# 2. Remove duplicates WITHIN each split\ntrain_df = train_df.drop_duplicates(subset='text')\nval_df = val_df.drop_duplicates(subset='text')\n\nprint(f\"After deduplication - Train: {train_df.shape}, Val: {val_df.shape}\")\n\n# 3. Check for any intersection (should be zero)\ntrain_texts_set = set(train_df['text'])\nval_texts_set = set(val_df['text'])\nduplicates = train_texts_set.intersection(val_texts_set)\nprint(f\"Number of duplicate texts between splits: {len(duplicates)}\")\n\n# 4. Extract the cleaned lists back for the rest of the pipeline\ntrain_texts = train_df['text'].tolist()\ntrain_labels = train_df['generated'].tolist()\nval_texts = val_df['text'].tolist()\nval_labels = val_df['generated'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.4 Tokenize the Text Data","metadata":{}},{"cell_type":"markdown","source":"1. The tokenizer for transformer model (distilroBerta) is loaded.\n2. This tokenizer will convert the raw text into numerical input IDs for the model to train.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\nmodel_checkpoint = \"distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.5 Convert to Hugging Face Dataset Format","metadata":{}},{"cell_type":"markdown","source":"The dataset in Python list is converted into Hugging Face's Dataset object to support .map() operations which enables the fast tokenization process.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\n\n# - Added deduplication\n# Convert to DataFrames to deduplicate\ntrain_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\nval_df = pd.DataFrame({'text': val_texts, 'label': val_labels})\n\n# Remove duplicates WITHIN each split to prevent data leakage\ntrain_df = train_df.drop_duplicates(subset='text')\nval_df = val_df.drop_duplicates(subset='text')\n\n# Convert the cleaned DataFrames to Hugging Face Datasets\ntrain_dataset_hf = Dataset.from_dict({'text': train_df['text'].tolist(), 'label': train_df['label'].tolist()})\nval_dataset_hf = Dataset.from_dict({'text': val_df['text'].tolist(), 'label': val_df['label'].tolist()})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.6 Tokenize the Dataset in Batches","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\n\n# Tokenize in batches (faster + uses multiple cores)\ndef tokenize_batch(example):\n    return tokenizer(\n        example['text'],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n\n# Apply tokenizer\ntrain_dataset_tokenized = train_dataset_hf.map(tokenize_batch, batched=True, num_proc=4)\nval_dataset_tokenized   = val_dataset_hf.map(tokenize_batch,   batched=True, num_proc=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. The dataset is tokenized in parallel using num_proc=4, improving the tokenization speed.\n   * import multiprocessing\n   * num_cores = multiprocessing.cpu_count()\n   * print(f\"Available CPU cores: {num_cores}\")\n   * **Use the above code to check the number of CPU core available (num_proc) and change accordingly.** \n2. Padding and Truncation are applied so all input sequences are the same length (512 tokens)","metadata":{}},{"cell_type":"markdown","source":"#### 3.7 Format the Dataset for PyTorch","metadata":{}},{"cell_type":"markdown","source":"The Hugging Face dataset is converted to PyTorch tensors so that it is ready to be used for PyTorch training.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ######################################### Format for PyTorch\ntrain_dataset_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.8 Create DataLoaders for Training","metadata":{}},{"cell_type":"markdown","source":"1. DataLoader objects are created to iterate over the batches during training and validation.\n2. \"shuffle=True\" helps in generalization during training.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\ntrain_loader = DataLoader(train_dataset_tokenized, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset_tokenized, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3.9 Sanity Check","metadata":{}},{"cell_type":"markdown","source":"Do checking to make sure tokenization worked as expected by printing the first few token IDs, attention mask values and label.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Khow Kai Yong 2105725\n# ########################################\nprint(\"Tokenization complete.\")\nsample = train_dataset_tokenized[0]\nprint(\"Sample input IDs:\", sample['input_ids'][:10])\nprint(\"Attention mask:\", sample['attention_mask'][:10])\nprint(\"Label:\", sample['label'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.0: Model Training & Finetuning**","metadata":{}},{"cell_type":"markdown","source":"This section documents how we fine-tune a pretrained transformer model (e.g., RoBERTa) for **binary classification** (0 = human, 1 = AI). ","metadata":{}},{"cell_type":"markdown","source":"Weights & Biases logging was disabled via environment variables to prevent external tracking, ensuring training remained local and reproducible without reliance on third-party logging services.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n# DISABLE WANDB COMPLETELY\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.1: Model Training","metadata":{}},{"cell_type":"markdown","source":"This section initializes and fine-tunes a RoBERTa-based sequence classification model using Hugging Faceâ€™s `Trainer`. \nIt sets up training arguments (epochs, batch size, learning rate, weight decay), defines evaluation metrics (accuracy, \nprecision, recall, F1), and manages dataset batching with a data collator. The `Trainer` handles optimization, validation \neach epoch, and saves the best checkpoint based on F1-score for later use.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n#TRAIN + SAVE + REPORT \nimport math, transformers, torch, inspect\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n\nprint(\"Transformers:\", transformers.__version__, \"| CUDA:\", torch.cuda.is_available())\n\n# Fallback if not defined earlier\nif 'model_checkpoint' not in globals():\n    model_checkpoint = \"roberta-base\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n\n# Version-adaptive eval key\nta_sig   = inspect.signature(TrainingArguments.__init__).parameters\neval_key = \"eval_strategy\" if \"eval_strategy\" in ta_sig else (\"evaluation_strategy\" if \"evaluation_strategy\" in ta_sig else None)\n\ntraining_kwargs = dict(\n    output_dir=\"./mini_results\",\n    num_train_epochs=2,                 \n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_strategy=\"epoch\" if \"save_strategy\" in ta_sig else \"steps\",\n    load_best_model_at_end=True if \"load_best_model_at_end\" in ta_sig else False,\n    metric_for_best_model=\"f1\" if \"metric_for_best_model\" in ta_sig else None,\n    greater_is_better=True if \"greater_is_better\" in ta_sig else None,\n    # Prefer \"none\" to silence W&B in newer transformers\n    report_to=\"none\" if \"report_to\" in ta_sig else None,\n    remove_unused_columns=True if \"remove_unused_columns\" in ta_sig else None,\n    dataloader_num_workers=2 if \"dataloader_num_workers\" in ta_sig else None,\n    dataloader_pin_memory=True if \"dataloader_pin_memory\" in ta_sig else None,\n    eval_accumulation_steps=4 if \"eval_accumulation_steps\" in ta_sig else None,\n)\nif eval_key:\n    training_kwargs[eval_key] = \"epoch\"\ntraining_kwargs = {k: v for k, v in training_kwargs.items() if v is not None}\nargs = TrainingArguments(**training_kwargs)\n\ndef compute_metrics(pred):\n    import numpy as np\n    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n    y_true = pred.label_ids\n    y_pred = pred.predictions.argmax(-1)\n    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n    acc = accuracy_score(y_true, y_pred)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": p, \"recall\": r}\n\n# processing_class (new) vs tokenizer (old)\ntrainer_kwargs = dict(\n    model=model,\n    args=args,\n    train_dataset=train_dataset_tokenized,\n    eval_dataset=val_dataset_tokenized,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\nif \"processing_class\" in inspect.signature(Trainer.__init__).parameters:\n    trainer_kwargs[\"processing_class\"] = tokenizer\nelse:\n    trainer_kwargs[\"tokenizer\"] = tokenizer\n\ntrainer = Trainer(**trainer_kwargs)\n\nsteps_per_epoch = math.ceil(len(train_dataset_tokenized) / args.per_device_train_batch_size)\nprint(f\"Train size: {len(train_dataset_tokenized)} | Val size: {len(val_dataset_tokenized)} | Steps/epoch: ~{steps_per_epoch}\")\n\ntrainer.train()\nprint(\"Best checkpoint:\", getattr(trainer.state, \"best_model_checkpoint\", None))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.2 Save Model","metadata":{}},{"cell_type":"markdown","source":"Save the trained model and tokenizer into a timestamped folder. \nIf a best checkpoint exists, it is copied; otherwise, the current model is saved. \nThe tokenizer is stored alongside to ensure reproducibility during inference.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n\nimport time, shutil\n# ---- Save best/current + tokenizer to timestamped folder ----\nts = time.strftime(\"%Y%m%d-%H%M%S\")\nsave_dir = f\"./mini_detector-{ts}\"\nbest_ckpt = getattr(trainer.state, \"best_model_checkpoint\", None)\nos.makedirs(save_dir, exist_ok=True)\nif best_ckpt and os.path.isdir(best_ckpt):\n    shutil.copytree(best_ckpt, save_dir, dirs_exist_ok=True)\nelse:\n    trainer.save_model(save_dir)\ntokenizer.save_pretrained(save_dir)\nprint(\"Saved to:\", save_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.3 Report","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Charmaine Hooi Wai Yee 2104533\n# ########################################\n\n# ---- Quick validation report ----\nimport numpy as np\npred = trainer.predict(val_dataset_tokenized)\nprobs = torch.softmax(torch.tensor(pred.predictions), dim=-1).numpy()\ny_true = pred.label_ids\ny_pred = probs.argmax(1)\ny_score = probs[:,1]\n\nprint(\"\\nClassification report (0=human, 1=ai):\\n\",\n      classification_report(y_true, y_pred, digits=4))\nprint(\"Confusion matrix [[TN,FP],[FN,TP]]:\\n\", confusion_matrix(y_true, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_true, y_score))\nprint(\"PR-AUC :\", average_precision_score(y_true, y_score))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.0: Model Testing & Analysis ##","metadata":{}},{"cell_type":"markdown","source":"#### 5.1: Load and Prepare Test Dataset","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\n# Load the original test split from the dataset\ntest_dataset_original = dataset['test']\ntest_df = pd.DataFrame(test_dataset_original)\n\n# Apply the same cleaning steps as training data\nprint(\"Original test dataset shape:\", test_df.shape)\nprint(\"Test set class distribution:\")\nprint(test_df['generated'].value_counts())\n\n# Clean the test data (same preprocessing as training)\ntest_df = test_df.drop_duplicates(subset='text')\ntest_df = test_df.dropna(subset=['text'])\ntest_df['text'] = test_df['text'].astype(str).str.strip()\ntest_df = test_df[test_df['text'] != \"\"]\ntest_df['text'] = test_df['text'].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n\nprint(\"\\nCleaned test dataset shape:\", test_df.shape)\nprint(\"Cleaned test set class distribution:\")\nprint(test_df['generated'].value_counts())\n\n# Convert to Hugging Face Dataset format\ntest_dataset_hf = Dataset.from_dict({\n    'text': test_df['text'].tolist(), \n    'label': test_df['generated'].tolist()\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.2: Tokenize Test Dataset","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\n# Use the same tokenization as training\ntest_dataset_tokenized = test_dataset_hf.map(tokenize_batch, batched=True, num_proc=4)\ntest_dataset_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(f\"Test dataset tokenized: {len(test_dataset_tokenized)} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.3: Evaluate Model on Test Set","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\n# Get predictions on test set\ntest_predictions = trainer.predict(test_dataset_tokenized)\n\n# Extract probabilities and predictions\ntest_probs = torch.softmax(torch.tensor(test_predictions.predictions), dim=-1).numpy()\ntest_y_true = test_predictions.label_ids\ntest_y_pred = test_probs.argmax(1)\ntest_y_score = test_probs[:, 1]  # Probability of AI class","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.4: Compute Performance Metrics for Train Set & Test Set","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    precision_score, recall_score, f1_score, roc_auc_score, \n    average_precision_score\n)\n\ndef evaluate_predictions(y_true, y_pred, y_score, split_name=\"Dataset\"):\n    metrics = {}\n    metrics['Accuracy']   = accuracy_score(y_true, y_pred)\n    metrics['Precision']  = precision_score(y_true, y_pred, average='binary')\n    metrics['Recall']     = recall_score(y_true, y_pred, average='binary')\n    metrics['F1-Score']   = f1_score(y_true, y_pred, average='binary')\n    metrics['ROC-AUC']    = roc_auc_score(y_true, y_score)\n    metrics['PR-AUC']     = average_precision_score(y_true, y_score)\n    \n    print(f\"\\n {split_name} RESULTS \")\n    print(f\"Accuracy:  {metrics['Accuracy']:.4f}\")\n    print(f\"Precision: {metrics['Precision']:.4f}\")\n    print(f\"Recall:    {metrics['Recall']:.4f}\")\n    print(f\"F1-Score:  {metrics['F1-Score']:.4f}\")\n    print(f\"ROC-AUC:   {metrics['ROC-AUC']:.4f}\")\n    print(f\"PR-AUC:    {metrics['PR-AUC']:.4f}\")\n    \n    print(\"\\nClassification Report (0=Human, 1=AI):\")\n    print(classification_report(y_true, y_pred, target_names=['Human','AI'], digits=4))\n    \n    print(\"Confusion Matrix [[TN, FP], [FN, TP]]:\")\n    print(confusion_matrix(y_true, y_pred))\n    \n    return metrics\n    \nprint(f\"Train Set Size: {len(train_dataset_tokenized):,} samples\")\nprint(f\"Val Set Size: {len(val_dataset_tokenized):,} samples\") \nprint(f\"Test Set Size: {len(test_dataset_tokenized):,} samples\")\n\ntrain_metrics = evaluate_predictions(y_true, y_pred, y_score, \"Train Set\")\ntest_metrics  = evaluate_predictions(test_y_true, test_y_pred, test_y_score, \"Test Set\")\n\n\n# Combine into a DataFrame for side-by-side comparison\ncomparison_df = pd.DataFrame([train_metrics, test_metrics], index=[\"Train\", \"Test\"])\nprint(\"\\n\")\ndisplay(comparison_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.5: Performance Visualization","metadata":{}},{"cell_type":"markdown","source":"Plot ROC and PR curves for both train & test","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\nplt.figure(figsize=(12, 5))\n\n# --- ROC Curve\nplt.subplot(1, 2, 1)\nfpr_train, tpr_train, _ = roc_curve(y_true, y_score)\nfpr_test,  tpr_test,  _ = roc_curve(test_y_true,  test_y_score)\n\nplt.plot(fpr_train, tpr_train, color='blue',  lw=2,\n         label=f'Train ROC (AUC = {train_metrics[\"ROC-AUC\"]:.3f})')\nplt.plot(fpr_test,  tpr_test,  color='red',   lw=2,\n         label=f'Test ROC (AUC = {test_metrics[\"ROC-AUC\"]:.3f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n\nplt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\n\n# --- Precision-Recall Curve\nplt.subplot(1, 2, 2)\nprec_train, rec_train, _ = precision_recall_curve(y_true, y_score)\nprec_test,  rec_test,  _ = precision_recall_curve(test_y_true,  test_y_score)\n\nplt.plot(rec_train, prec_train, color='blue', lw=2,\n         label=f'Train PR (AUC = {train_metrics[\"PR-AUC\"]:.3f})')\nplt.plot(rec_test,  prec_test,  color='red',  lw=2,\n         label=f'Test PR (AUC = {test_metrics[\"PR-AUC\"]:.3f})')\n\nplt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\nplt.xlabel('Recall'); plt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower right\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.6: Cross Domain Testing","metadata":{}},{"cell_type":"markdown","source":"For Cross-Domain Testing, we loaded the Kaggle dataset AH&AITD dataset, which is larger, more diverse, and includes formal texts from older sources than the originalÂ dataset.","metadata":{}},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\nimport pandas as pd\n\n# Load the Excel file\ncross_domain_df = pd.read_excel(\n    \"/kaggle/input/ah-and-aitd-arslans-human-and-ai-text-database/Dataset.xlsx\"\n)\n\nprint(test_df.head())\nprint(test_df.columns)\nprint(test_df['label_name'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\n# Convert to Hugging Face Dataset format\ncross_domain_dataset = Dataset.from_dict({\n    'text': cross_domain_df['text'].tolist(),\n    'label': cross_domain_df['label_id'].tolist()   # use label_id instead of label\n})\n\n# Tokenize with the same tokenizer used in training\ncross_domain_tokenized = cross_domain_dataset.map(\n    lambda x: tokenizer(x['text'], padding=\"max_length\", truncation=True),\n    batched=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\ncross_pred = trainer.predict(cross_domain_tokenized)\n\n# Convert logits to probabilities\ncross_probs = torch.softmax(torch.tensor(cross_pred.predictions), dim=-1).numpy()\ncross_y_true = cross_pred.label_ids\ncross_y_pred = cross_probs.argmax(1)\ncross_y_score = cross_probs[:, 1]  # probability of class \"AI\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\ncross_metrics = evaluate_predictions(cross_y_true, cross_y_pred, cross_y_score, split_name=\"Cross-Domain Test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ########################################\n# Coded By: Seow Yi Xuan 2105524\n# ########################################\n\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\n# ROC curve\nfpr_cross, tpr_cross, _ = roc_curve(cross_y_true, cross_y_score)\nfpr_in, tpr_in, _ = roc_curve(test_y_true, test_y_score)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr_in, tpr_in, label=f\"In-domain ROC (AUC={test_metrics['ROC-AUC']:.3f})\", color=\"blue\")\nplt.plot(fpr_cross, tpr_cross, label=f\"Cross-domain ROC (AUC={cross_metrics['ROC-AUC']:.3f})\", color=\"red\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\n\n# PR curve\nprec_cross, rec_cross, _ = precision_recall_curve(cross_y_true, cross_y_score)\nprec_in, rec_in, _ = precision_recall_curve(test_y_true, test_y_score)\n\nplt.subplot(1, 2, 2)\nplt.plot(rec_in, prec_in, label=f\"In-domain PR (AUC={test_metrics['PR-AUC']:.3f})\", color=\"blue\")\nplt.plot(rec_cross, prec_cross, label=f\"Cross-domain PR (AUC={cross_metrics['PR-AUC']:.3f})\", color=\"red\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}